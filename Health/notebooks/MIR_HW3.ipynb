{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dul1eni14UhG"
      },
      "source": [
        "# <strong><font color=#A52A2A></span> MIR_HW3 </strong>\n",
        "\n",
        "98101566 -> Mohammadreza Daviran\n",
        "<br>\n",
        "98105919 -> Reza Erfan Arani\n",
        "<br>\n",
        "98106434 -> Mohammad Khodadadi Aski\n",
        "<br>\n",
        "\n",
        "<strong><font color=#B22222></span> Please run this notebook on google colab! </strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTVGDb3hIVmX"
      },
      "source": [
        "# <strong><font color=#5F9EA0></span> Download the files </strong> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x1_NB3_NOVl"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P93rCnrQNqAk",
        "outputId": "cba23be0-ace6-44da-825e-d4717ca560c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH7qogTpOdBg",
        "outputId": "31c18c58-1891-4912-93f1-81e17b4a8fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'course-nlp-ir-1-text-exploring'...\n",
            "remote: Enumerating objects: 401, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 401 (delta 2), reused 0 (delta 0), pack-reused 397\u001b[K\n",
            "Receiving objects: 100% (401/401), 98.78 MiB | 13.75 MiB/s, done.\n",
            "Resolving deltas: 100% (197/197), done.\n",
            "Checking out files: 100% (245/245), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/language-ml/course-nlp-ir-1-text-exploring.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXIGBR5SOgjd",
        "outputId": "ec14e8a3-8670-4ea1-9981-c66ec28af838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/course-nlp-ir-1-text-exploring/exploring-datasets/health\n"
          ]
        }
      ],
      "source": [
        "%cd course-nlp-ir-1-text-exploring/exploring-datasets/health"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv7vDB6ZO1ip",
        "outputId": "f6c1dadc-d895-4b49-b70e-39248eeaba22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidoctor-1.json  hidoctor-4.json  namnak-2.json  namnak-5.json\n",
            "hidoctor-2.json  hidoctor-5.json  namnak-3.json  README.md\n",
            "hidoctor-3.json  namnak-1.json\t  namnak-4.json\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhuA9EIbpf4C"
      },
      "source": [
        "Alternative way to download the files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yJGTfwvFPie"
      },
      "outputs": [],
      "source": [
        "# files = []\n",
        "# result = list()\n",
        "# for i in range(1, 6):\n",
        "#   link = \"https://github.com/language-ml/course-nlp-ir-1-text-exploring/blob/main/exploring-datasets/health/hidoctor-\" + str(i) + \".json\"\n",
        "#   hidoctor = requests.get(link, allow_redirects=True)\n",
        "#   name = 'hidoctor-' + str(i) + '.json'\n",
        "#   temp_file = open(name, 'wb').write(hidoctor.content)\n",
        "#   files.append(name)\n",
        "\n",
        "\n",
        "# for i in range(1, 6):\n",
        "#   link = \"https://github.com/language-ml/course-nlp-ir-1-text-exploring/blob/main/exploring-datasets/health/namnak-\" + str(i) + \".json\"\n",
        "#   namnak = requests.get(link, allow_redirects=True)\n",
        "#   name = 'namnak-' + str(i) + '.json'\n",
        "#   temp_file = open(name, 'wb').write(namnak.content)\n",
        "#   files.append(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2NrMF9_pklF"
      },
      "source": [
        "Our dataset is json so first of all we merge all the files to 'bio.json' file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTYq4-VI7WW9"
      },
      "outputs": [],
      "source": [
        "def merge_JsonFiles(filename):\n",
        "    result = list()\n",
        "    for f1 in filename:\n",
        "        with open(f1, 'r') as infile:\n",
        "            result.extend(json.load(infile))\n",
        "\n",
        "    with open('bio.json', 'w') as output_file:\n",
        "        json.dump(result, output_file)\n",
        "\n",
        "# files = ['hidoctor-1.json', 'hidoctor-2.json', 'hidoctor-3.json','hidoctor-4.json','hidoctor-5.json', 'namnak-1.json', 'namnak-2.json', 'namnak-3.json','namnak-4.json','namnak-5.json']\n",
        "files = ['namnak-1.json', 'namnak-2.json', 'namnak-3.json','namnak-4.json','namnak-5.json']\n",
        "merge_JsonFiles(files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeRQB1m1pwB9"
      },
      "source": [
        "By loading the json file, we extract all the paragraphs and save them in *documents* for future work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qsxSn8lCfMD"
      },
      "outputs": [],
      "source": [
        "f = open('bio.json')\n",
        "bioset = json.load(f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K604XMuY39K"
      },
      "outputs": [],
      "source": [
        "documents = []\n",
        "for i in bioset:\n",
        "  documents.append(i['paragraphs'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZWE3sU4JLjY"
      },
      "source": [
        "# <strong><font color=#5F9EA0></span> Preprocessing </strong> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuRFsfnEqqOk"
      },
      "source": [
        "First we import the requirements for preprocessing the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnMeJTB7QQcL"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import tqdm\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk import word_tokenize\n",
        "import csv\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import random \n",
        "import nltk\n",
        "import codecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "LEp1MQ0PVKjd",
        "outputId": "5363f9ff-289b-45ee-9c88-9eb6bf3529f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 7.6 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 55.0 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 55.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394486 sha256=6fa6e724d21ac116b6353994c182f35de1d9291e4569b84cbeb26fc7d482e7c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154945 sha256=edda26e1298907d18b9f3a6f2214cf45e0bb66557e50de7ef0ab8f1f16b32758\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX9SSCH-Kqpc"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlOB6aLdaxLf"
      },
      "source": [
        "**normalize**\n",
        "\n",
        "normalizer function normilizes the docs we extracted and saves them \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCDBAkhV1z7N",
        "outputId": "c8f8782d-e4d0-4c23-9ca7-fee1bd1ef73d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 875/875 [00:02<00:00, 295.17it/s]\n"
          ]
        }
      ],
      "source": [
        "normalizer = Normalizer()\n",
        "\n",
        "doc_normalized = [[normalizer.normalize(y) for y in x] for x in tqdm.tqdm(documents)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4F3QHC8UhbPY"
      },
      "outputs": [],
      "source": [
        "doc_normalized_edited = []\n",
        "for doc in doc_normalized:\n",
        "  temp = []\n",
        "  for x in doc:\n",
        "    temp.append(x.replace('\\u200c', ' '))\n",
        "  doc_normalized_edited.append(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krf6gcD-13EE"
      },
      "source": [
        "**wording**\n",
        "\n",
        "here we tokenize the sentences in each normilized document and then we save them in the doc_sentences array "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGhgVtpT1nFH",
        "outputId": "4a75f0f7-0793-4596-f05d-842899f03a5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 875/875 [00:00<00:00, 3560.10it/s]\n"
          ]
        }
      ],
      "source": [
        "doc_sentences = [sent_tokenize(' '.join(x)) for x in tqdm.tqdm(doc_normalized_edited)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTea4fdk17ZQ"
      },
      "source": [
        "**tokenize**\n",
        "\n",
        "we tokenize the words in the doc_sentences array and the we save them into doc_tokens array by tqdm tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ah83qkI1vjg",
        "outputId": "6344556f-65db-4234-d0aa-6cbc2d8d2a80"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 875/875 [00:01<00:00, 493.56it/s]\n"
          ]
        }
      ],
      "source": [
        "doc_tokens = [[word_tokenize(sent) for sent in sents] for sents in tqdm.tqdm(doc_sentences)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LCwvfkRb7Hy"
      },
      "source": [
        "Then by using itertools save them in all_tokens. Because we want to exclude the stopwords from our tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtPXv8ZRIuQx"
      },
      "outputs": [],
      "source": [
        "all_tokens = []\n",
        "for i in doc_tokens:\n",
        "  all_tokens.append(list(itertools.chain.from_iterable(i)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAcWcUdIH86r"
      },
      "source": [
        "**Delete stop words**\n",
        "\n",
        "here we \n",
        "\n",
        "\n",
        "1.   download the persian stopwords from bellow link\n",
        "2.   then we delete them from all_tokens arraylist cause they have no value for us\n",
        "3. save the tokens which are not stopwords in all_tokens_nonstop arraylist\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fBlxGsBA0bH"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "link = \"https://raw.githubusercontent.com/sobhe/hazm/master/hazm/data/stopwords.dat\"\n",
        "temp_file = open('stopwords.txt', 'wb').write(requests.get(link, allow_redirects=True).content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "getNj9JSA-7z"
      },
      "outputs": [],
      "source": [
        "stopwords = [normalizer.normalize(x.strip()) for x in codecs.open('stopwords.txt','r','utf-8').readlines()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udEmVGOGI3c-"
      },
      "outputs": [],
      "source": [
        "all_tokens_nonstop = []\n",
        "for doc in all_tokens:\n",
        "  all_tokens_nonstop.append([t for t in tqdm.tqdm(doc) if t not in stopwords])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jQZmE4hMhXr"
      },
      "source": [
        "**lemmatization, stemming**\n",
        "\n",
        "here we lemmatize and stem the tokens "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATzrOSutMijX"
      },
      "outputs": [],
      "source": [
        "stemmer = Stemmer()\n",
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "def get_lemma_set(tok, opt=1):\n",
        "    if opt == 1:\n",
        "        return stemmer.stem(tok)\n",
        "    if opt == 2:\n",
        "        return lemmatizer.lemmatize(tok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dst1LATNMvRu"
      },
      "outputs": [],
      "source": [
        "opt = 2\n",
        "\n",
        "all_tokens_nonstop_lemstem = []\n",
        "\n",
        "for doc in all_tokens_nonstop:\n",
        "  all_tokens_nonstop_lemstem.append([get_lemma_set(t, opt) for t in tqdm.tqdm(doc)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3z__GsXjc0B"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "lists = np.array(all_tokens_nonstop_lemstem).tolist()\n",
        "json_str = json.dumps(lists)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bERcIyWn5x_N"
      },
      "outputs": [],
      "source": [
        "data = json.loads(json_str)\n",
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdrAW7dhmuD7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('all_tokens_nonstop_lemstem.json', 'w') as o:\n",
        "    json.dump(json_str, o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFWisBl1mHlU"
      },
      "outputs": [],
      "source": [
        "all_tokens_nonstop_lemstem[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bub62TFX317z"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JMG1CXr0MCK"
      },
      "outputs": [],
      "source": [
        "%cd sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4N1ereaaj3NV"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('all_tokens_nonstop_lemstem.json', 'r') as input_file:\n",
        "    all_tokens_nonstop_lemstem = json.load(input_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRoetcPv6b6F"
      },
      "outputs": [],
      "source": [
        "all_tokens_nonstop_lemstem = json.loads(all_tokens_nonstop_lemstem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxhmfDta6jN0"
      },
      "outputs": [],
      "source": [
        "all_tokens_nonstop_lemstem[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YR336LGSfEqk"
      },
      "outputs": [],
      "source": [
        "from numpy import asarray\n",
        "from numpy import savetxt\n",
        "\n",
        "data = asarray(all_tokens_nonstop_lemstem)\n",
        "\n",
        "savetxt('data.csv', data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdFq0jEPi4lD"
      },
      "outputs": [],
      "source": [
        "all_tokens_nonstop_lemstem[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-R2ZZL_8PXl"
      },
      "source": [
        "# <strong><font color=#5F9EA0></span> Query Expansion </strong> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h76hvRcYCfOI"
      },
      "source": [
        "## <strong><font color=#AF8E38></span> Download Synonym's Dataset </strong> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4sOTG1uB9F7"
      },
      "source": [
        "First we use a dataset for synonyms that we have downloaded from FarsNet.\n",
        "\n",
        "(We have uploded the dataset on google drive so we have to download it before anything else)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKwsdw6CAoNK"
      },
      "outputs": [],
      "source": [
        "# !pip install gdown\n",
        "import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hwBQlE78O3S"
      },
      "outputs": [],
      "source": [
        "dictLink = 'https://drive.google.com/file/d/1g13xPperfr-ci0jxEEGzAoY6Fmx8Ie6T/view?usp=sharing'\n",
        "dictLinkID = 'https://drive.google.com/uc?id=1g13xPperfr-ci0jxEEGzAoY6Fmx8Ie6T'\n",
        "dictLink2 = 'https://drive.google.com/u/0/uc?id=1g13xPperfr-ci0jxEEGzAoY6Fmx8Ie6T&export=download'\n",
        "output = 'data.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "q4EuQCxM8p3W",
        "outputId": "0ea87359-746b-4c09-9079-419d6c6d3680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1g13xPperfr-ci0jxEEGzAoY6Fmx8Ie6T\n",
            "To: /content/course-nlp-ir-1-text-exploring/exploring-datasets/health/data.pkl\n",
            "100%|██████████| 2.28M/2.28M [00:00<00:00, 60.8MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data.pkl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "gdown.download(dictLinkID, output, quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRFSfU97CW7S"
      },
      "source": [
        "## <strong><font color=#AF8E38></span> Read Query </strong> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsfkzNcGGLkK"
      },
      "source": [
        "Read the query that we want to search later in documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZo-FbSbCXSt",
        "outputId": "09f18461-3297-4e04-d1da-f9d9ebf87cbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search...  دکتر قلب کرونا\n",
            "['دکتر', 'قلب', 'کرونا']\n"
          ]
        }
      ],
      "source": [
        "query = input('Search...  ')\n",
        "queryList = query.split()\n",
        "print(queryList)\n",
        "expandedQuery = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75T58fLCGE17"
      },
      "source": [
        "## <strong><font color=#AF8E38></span> Find Synonyms </strong> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0_JFZPKGa0P"
      },
      "source": [
        "From the dataset we have we find synonyms for all words in the query and append them to the query that we had."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut3tLtOMDf-k"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjNA6TRMDIP8",
        "outputId": "0735d5fd-c8af-4870-802a-0e9d93db2c4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "دکتر\n",
            "قلب\n",
            "کرونا\n"
          ]
        }
      ],
      "source": [
        "expand = ''\n",
        "for x in queryList:\n",
        "    if (x in stopwords):\n",
        "      continue\n",
        "    input_query = x\n",
        "    pkl_file = open('data.pkl', 'rb')\n",
        "    dic = pickle.load(pkl_file)\n",
        "    pkl_file.close()\n",
        "    normalizer = Normalizer()\n",
        "    normalized = normalizer.normalize(input_query)\n",
        "    print(normalized)\n",
        "    # stemmer = Stemmer()\n",
        "    # stemmed = stemmer.stem(normalized)\n",
        "    # tokens = word_tokenize(stemmed)\n",
        "    list_of_synonyms = {}\n",
        "    try:\n",
        "        tokens_synonyms = word_tokenize(dic[x])\n",
        "        listSyn = []\n",
        "        for y in tokens_synonyms:\n",
        "          listSyn.append(y)\n",
        "        list_of_synonyms[x] = listSyn\n",
        "    except:\n",
        "        pass\n",
        "        # list_of_synonyms[x] = x\n",
        "    values = list(list_of_synonyms.values())\n",
        "    try:\n",
        "      values = values[0]\n",
        "      expand += f'{values[0]} '\n",
        "    except:\n",
        "      expand += ''\n",
        "expand = expand.strip()\n",
        "# print(expand[1])\n",
        "expandedQuery = f'{query} {expand}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hRzsV3PhDl4o",
        "outputId": "2e22d64b-ddbe-4ec7-d2a2-fa0ec84c401b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'دکتر قلب کرونا پزشک دل'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "expandedQuery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVlyHY3kKrhK"
      },
      "source": [
        "# <strong><font color=#5F9EA0></span> Boolean retrieval </strong> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgIHcLbxFuYo"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize , word_tokenize\n",
        "import glob\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqZvCNbNjxec"
      },
      "source": [
        "all words longer than 2 characters are stored in bag_of_words array \n",
        "all unique words in the bag of words set are stored in unique_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFjYDWijHRgR"
      },
      "outputs": [],
      "source": [
        "  bag_of_words = []\n",
        "  unique_words = set()\n",
        "  lemmatized_docs = [' '.join(x) for x in all_tokens_nonstop_lemstem]\n",
        "  for doc in lemmatized_docs:\n",
        "    temp = doc.split(' ')\n",
        "    bag_of_word = [t for t in temp if len(t) > 2]\n",
        "    bag_of_words.append(bag_of_word)\n",
        "    unique_words = set(unique_words).union(set(bag_of_word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBouGczZkKhA"
      },
      "source": [
        "we specify which words from unique_words each document contains and store their number in num_of_words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laZZoggBJyIe"
      },
      "outputs": [],
      "source": [
        "num_of_words = []\n",
        "\n",
        "for i in range(len(bag_of_words)):\n",
        "  numOfWord = dict.fromkeys(unique_words, 0)\n",
        "  for word in bag_of_words[i]:\n",
        "    numOfWord[word] += 1\n",
        "  num_of_words.append(numOfWord)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ruonMSWtDzF"
      },
      "source": [
        "We illustrate the num_of_words list for the first 10 documents for better comprehension:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "7StRxbDuMLBn",
        "outputId": "9bff495c-0add-4bea-fc50-278471236a4d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-50fb3122-0afb-4623-af86-634c34730be2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>روزمانع</th>\n",
              "      <th>برنجی</th>\n",
              "      <th>شکستن</th>\n",
              "      <th>آبمیوه‎</th>\n",
              "      <th>استافیلوکوک</th>\n",
              "      <th>رابینسون</th>\n",
              "      <th>تیروئید</th>\n",
              "      <th>انحصار</th>\n",
              "      <th>ممتاز</th>\n",
              "      <th>جلوه</th>\n",
              "      <th>...</th>\n",
              "      <th>آردسر</th>\n",
              "      <th>شناسنامه</th>\n",
              "      <th>شرمنده</th>\n",
              "      <th>وان</th>\n",
              "      <th>Primrose</th>\n",
              "      <th>زعفران</th>\n",
              "      <th>چرخان</th>\n",
              "      <th>یاعلائمی</th>\n",
              "      <th>پارانوئید,</th>\n",
              "      <th>عادات</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 16887 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50fb3122-0afb-4623-af86-634c34730be2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-50fb3122-0afb-4623-af86-634c34730be2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-50fb3122-0afb-4623-af86-634c34730be2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   روزمانع  برنجی  شکستن  آبمیوه‎  استافیلوکوک  رابینسون  تیروئید  انحصار  \\\n",
              "0        0      0      0        0            0         0        0       0   \n",
              "1        0      0      0        0            0         0        0       0   \n",
              "2        0      0      1        0            0         0        0       0   \n",
              "3        0      0      0        0            0         0        0       0   \n",
              "4        0      0      0        0            0         0        0       0   \n",
              "5        0      0      0        0            0         0        0       0   \n",
              "6        0      0      0        0            0         0        0       0   \n",
              "7        0      0      0        0            0         0        0       0   \n",
              "8        0      0      0        0            0         0        0       0   \n",
              "9        0      0      0        0            0         0        0       0   \n",
              "\n",
              "   ممتاز  جلوه  ...  آردسر  شناسنامه  شرمنده  وان  Primrose  زعفران  چرخان  \\\n",
              "0      0     0  ...      0         0       0    0         0       0      0   \n",
              "1      0     0  ...      0         0       0    0         0       0      0   \n",
              "2      0     1  ...      0         0       0    0         0       0      0   \n",
              "3      0     0  ...      0         0       0    0         0       0      0   \n",
              "4      0     0  ...      0         0       0    0         0       0      0   \n",
              "5      0     0  ...      0         0       0    0         0       0      0   \n",
              "6      0     0  ...      0         0       0    0         0       0      0   \n",
              "7      0     0  ...      0         0       0    0         0       0      0   \n",
              "8      0     0  ...      0         0       0    0         0       0      0   \n",
              "9      0     0  ...      0         0       0    0         0       0      0   \n",
              "\n",
              "   یاعلائمی  پارانوئید,  عادات  \n",
              "0         0           0      0  \n",
              "1         0           0      0  \n",
              "2         0           0      0  \n",
              "3         0           0      0  \n",
              "4         0           0      0  \n",
              "5         0           0      0  \n",
              "6         0           1      0  \n",
              "7         0           0      0  \n",
              "8         0           0      0  \n",
              "9         0           0      0  \n",
              "\n",
              "[10 rows x 16887 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Total number of columns (16887) exceeds max_columns (20) limiting to first (20) columns.\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(num_of_words[0:10])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6WMgChqt0xZ"
      },
      "source": [
        "First we normalize and lemmatize our query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqErycYqbLMp"
      },
      "outputs": [],
      "source": [
        "stopwords = [normalizer.normalize(x.strip()) for x in codecs.open('stopwords.txt','r','utf-8').readlines()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhkYEGewEUc4"
      },
      "outputs": [],
      "source": [
        "input = 'علل ضعف بینایی'\n",
        "splitted_input = input.split(\" \")\n",
        "nsi = []\n",
        "k = 12\n",
        "for x in splitted_input:\n",
        "  if (x not in stopwords and len(x) > 2):\n",
        "    nsi.append(normalizer.normalize(lemmatizer.lemmatize(x))) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euV4K8as9lek",
        "outputId": "9fd8173d-bf7d-4ba7-d5be-ef0f5b823dac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['علل', 'ضعف', 'بینایی']"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nsi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSxzF1uDuDtg"
      },
      "source": [
        "To compare the query with our documents, we just need to find out in which documents the words of our query are. So we assign score for each doc with respect to the existence of words and the number of them. \n",
        "\n",
        "<strong><font color=#cf6679></span> We consider the number of words, so our implemented method is better than the ordinary Boolean method. If we want to implement ordinary boolean method we simply can comment one line!</strong> \n",
        "\n",
        "if you want to make the algorithm better , you can assume a negative point for not having a word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys8OmftTt8QK"
      },
      "outputs": [],
      "source": [
        "score = {}\n",
        "for i in range(len(all_tokens_nonstop_lemstem)):\n",
        "  score[i] = 0\n",
        "\n",
        "counter = 0\n",
        "for doc in num_of_words:\n",
        "  for word in nsi:\n",
        "    try :\n",
        "      if doc[word] > 0:\n",
        "        score[counter] += 1\n",
        "        # you can comment the next line\n",
        "        score[counter] += 0.02 * doc[word]\n",
        "    except :\n",
        "      pass\n",
        "  counter += 1\n",
        "sorted_score = {k: v for k, v in sorted(score.items(), key=lambda item: item[1], reverse=True)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXuPEBPaJgjk",
        "outputId": "78e3c6e5-fb68-4531-f36d-811bf6c779e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "آتروفی یا تحلیل عضلات: علل، علائم، پیشگیری و درمان\n",
            "https://www.hidoctor.ir/328173_%d8%a2%d8%aa%d8%b1%d9%88%d9%81%db%8c-%db%8c%d8%a7-%d8%aa%d8%ad%d9%84%db%8c%d9%84-%d8%b9%d8%b6%d9%84%d8%a7%d8%aa.html/\n",
            "\n",
            "اطلاعات جامع در مورد بیماری منییر\n",
            "https://www.hidoctor.ir/276282_%d8%a7%d8%b7%d9%84%d8%a7%d8%b9%d8%a7%d8%aa-%d8%ac%d8%a7%d9%85%d8%b9-%d8%af%d8%b1-%d9%85%d9%88%d8%b1%d8%af-%d8%a8%db%8c%d9%85%d8%a7%d8%b1%db%8c-%d9%85%d9%86%db%8c%db%8c%d8%b1.html/\n",
            "\n",
            "هورمون رشد خوبه یا بد؟\n",
            "https://namnak.com/هورمون-رشد.p33051\n",
            "\n",
            "اطلاعاتی در مورد سرطان و دیابت\n",
            "https://www.hidoctor.ir/275852_%d8%a7%d8%b7%d9%84%d8%a7%d8%b9%d8%a7%d8%aa%db%8c-%d8%af%d8%b1-%d9%85%d9%88%d8%b1%d8%af-%d8%b3%d8%b1%d8%b7%d8%a7%d9%86-%d9%88-%d8%af%db%8c%d8%a7%d8%a8%d8%aa.html/\n",
            "\n",
            "سرمه کشیدن و این همه خاصیت برای سلامت چشم\n",
            "https://www.hidoctor.ir/285564_%d8%b3%d8%b1%d9%85%d9%87-%da%a9%d8%b4%db%8c%d8%af%d9%86-%d9%88-%d8%a7%db%8c%d9%86-%d9%87%d9%85%d9%87-%d8%ae%d8%a7%d8%b5%db%8c%d8%aa-%d8%a8%d8%b1%d8%a7%db%8c-%d8%b3%d9%84%d8%a7%d9%85%d8%aa-%da%86%d8%b4.html/\n",
            "\n",
            "جراحی ترمیمی مجاری ادراری + راهنمایی کامل\n",
            "https://www.hidoctor.ir/345053_%d9%85%d8%ac%d8%a7%d8%b1%db%8c-%d8%a7%d8%af%d8%b1%d8%a7%d8%b1%db%8c.html/\n",
            "\n",
            "نحوه مصرف ویتامین E\n",
            "https://www.hidoctor.ir/256262_%d9%86%d8%ad%d9%88%d9%87-%d9%85%d8%b5%d8%b1%d9%81-%d9%88%db%8c%d8%aa%d8%a7%d9%85%db%8c%d9%86-e.html/\n",
            "\n",
            "عرق سرد و این خطرات\n",
            "https://www.hidoctor.ir/135026_%d8%b9%d8%b1%d9%82-%d8%b3%d8%b1%d8%af-%d9%88-%d8%a7%db%8c%d9%86-%d8%ae%d8%b7%d8%b1%d8%a7%d8%aa.html/\n",
            "\n",
            "شناسایی سلامتی بدن با عملکرد چشم\n",
            "https://namnak.com/عملکرد-چشم.p43729\n",
            "\n",
            "گرفتگی عضلات نشان دهنده چه مشکلاتی است؟ + علت\n",
            "https://namnak.com/گرفتگی-عضلات.p42123\n",
            "\n",
            "فلج مغزی چیست؟ علائم، علل، درمان بیماری CP\n",
            "https://www.hidoctor.ir/337767_%d9%81%d9%84%d8%ac-%d9%85%d8%ba%d8%b2%db%8c.html/\n",
            "\n",
            "فتوفوبیا یا حساسیت چشم به نور وقتی جدی می شود\n",
            "https://namnak.com/فتوفوبیا.p2610\n",
            "\n"
          ]
        }
      ],
      "source": [
        "counter = 0\n",
        "for x in sorted_score:\n",
        "  if counter >= k:\n",
        "    break\n",
        "  print(bioset[x]['title'])\n",
        "  print(bioset[x]['link'])\n",
        "  print()\n",
        "  counter += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PODaDRrrLGkE"
      },
      "source": [
        "**Alternative solution**\n",
        "\n",
        "we can do all these things and findout the scores without num_of_words.\n",
        "here is alternative solution for this problem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVlObgPZnqKE"
      },
      "outputs": [],
      "source": [
        "# input = input(\"Enter your subjects :\")\n",
        "input = 'ارتباط حمله قلبی و کرونا'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frSnBKUGAx8I"
      },
      "outputs": [],
      "source": [
        "score = []\n",
        "def reset_score():\n",
        "  for i in range(len(all_tokens_nonstop_lemstem)):\n",
        "    score.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEcOhn9yA0LO"
      },
      "outputs": [],
      "source": [
        "splitted_input = input.split(\" \")\n",
        "nsi = []\n",
        "for x in splitted_input:\n",
        "  if x not in stopwords:\n",
        "    nsi.append(normalizer.normalize(lemmatizer.lemmatize(x))) \n",
        "reset_score()\n",
        "counter = 0\n",
        "for doc in all_tokens_nonstop_lemstem :\n",
        "  for x in nsi:\n",
        "    if x in doc:\n",
        "      score[counter]+=1\n",
        "  counter += 1\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-liM9gvK8pd"
      },
      "source": [
        "# <strong><font color=#5F9EA0></span> tf−idf </strong> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMyuZouj3D89"
      },
      "source": [
        "Download all the requirements especially TfidfVectorizer from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3yUxe2KOG-E"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('all_tokens_nonstop_lemstem.json', 'r') as input_file:\n",
        "    all_tokens_nonstop_lemstem =  json.loads(json.load(input_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfmrYNpbd96L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w5tTod5nI24",
        "outputId": "f3e79535-4a32-4e16-ede8-66c30760296f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYJHapkznect"
      },
      "source": [
        "We use TfidfVectorizer with (1, 2) ngram and norm l2 for our vectorizer.\n",
        "Then we find all the documents' vectors with fit_transform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNpPBXvS200s"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(use_idf = True, norm ='l2', ngram_range=(1,2), analyzer='word')\n",
        "doc_term_mat = vectorizer.fit_transform([' '.join(doc) for doc in all_tokens_nonstop_lemstem])\n",
        "vocabulary = vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ynWJASGfl20"
      },
      "source": [
        "In this function we enter the query and do all the preprocessing on that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXEnY8WXTcmg"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "import nltk\n",
        "from hazm import *\n",
        "\n",
        "normalizer = Normalizer()\n",
        "stemmer = Stemmer()\n",
        "lemmatizer = Lemmatizer()\n",
        "stopwords = [normalizer.normalize(x.strip()) for x in codecs.open('stopwords.txt', 'r', 'utf-8').readlines()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J34mXjV0ZuTF"
      },
      "outputs": [],
      "source": [
        "def process_query(query):\n",
        "  splitted_input = query.split(\" \")\n",
        "  nsi = []\n",
        "  for x in splitted_input:\n",
        "    if x not in stopwords:\n",
        "      nsi.append([normalizer.normalize(lemmatizer.lemmatize(x))])\n",
        "  return nsi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkl2eLi-fwLf"
      },
      "source": [
        "To compare the query with our documents we need to find query_vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PjXY3pWZJEs"
      },
      "outputs": [],
      "source": [
        "def find_query_vector(tokens):\n",
        "  vector = np.zeros(len(vocabulary))\n",
        "  for token in itertools.chain(*tokens):\n",
        "      try:\n",
        "          index = vectorizer.vocabulary_[token]\n",
        "          vector[index] = 1\n",
        "      except ValueError:\n",
        "          pass\n",
        "  return vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8Ftuyepf5m6"
      },
      "source": [
        "After calculating query_vector, we need To see how close one doc is to the query. So we find the cosine distant between query_vector and each doc. Then we sort the indexes according to the scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJF69CQWZ2Qv"
      },
      "outputs": [],
      "source": [
        "def search(query, k):\n",
        "  scores = []\n",
        "  tokens = process_query(query)\n",
        "  query_vector = find_query_vector(tokens)\n",
        "\n",
        "  for doc in doc_term_mat.A:\n",
        "    scores.append(np.dot(query_vector, doc)/(np.linalg.norm(query_vector)*np.linalg.norm(doc)))\n",
        "  return np.array(scores).argsort()[-k:][::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxbAFIT9UAGo"
      },
      "outputs": [],
      "source": [
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo37LTzlahct"
      },
      "outputs": [],
      "source": [
        "query = \"نشانه های سکته مغزی\"\n",
        "x = search(query, k = 12)\n",
        "for index in x:\n",
        "  print(bioset[index]['title'])\n",
        "  print(bioset[index]['link'])\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRupn88kLDry"
      },
      "source": [
        "# <strong><font color=#5F9EA0></span> Transformer </strong> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_um3zTnLjgp"
      },
      "source": [
        "In this section we use trasformer models to embe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVdrYot_md0Z",
        "outputId": "2f71720c-6c2e-4009-d903-28bbf15263d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 5.5 MB/s \n",
            "\u001b[?25hCollecting faiss\n",
            "  Downloading faiss-1.5.3-cp37-cp37m-manylinux1_x86_64.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.0+cu113)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 43.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 10.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, faiss\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed faiss-1.5.3 huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers faiss torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idzLBNvALGT-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel, BigBirdModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO9j-x0GOac5"
      },
      "source": [
        "Check if gpu is available or not:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1M-X_Ove0Wm",
        "outputId": "b4cc0ff0-9113-45e8-d835-a4e35de6d81e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DbNe3GcPBx4"
      },
      "source": [
        "There are not many Persian models for Transformers. \n",
        "\n",
        "For this reason, we use one of the two models, **Parsbert** or **ParsBigBird**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzi0Aq6hPBca"
      },
      "outputs": [],
      "source": [
        "choice = 2\n",
        "\n",
        "if choice == 1:\n",
        "  model_name_or_path = \"HooshvareLab/bert-fa-zwnj-base\"\n",
        "  config = AutoConfig.from_pretrained(model_name_or_path)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "  # model = TFAutoModel.from_pretrained(model_name_or_path).to(device)\n",
        "  model = AutoModel.from_pretrained(model_name_or_path).to(device)\n",
        "\n",
        "if choice == 2:\n",
        "\n",
        "  MODEL_NAME = \"SajjadAyoubi/distil-bigbird-fa-zwnj\"\n",
        "\n",
        "  # model = BigBirdModel.from_pretrained(MODEL_NAME, block_size=32).to(device)\n",
        "  model = BigBirdModel.from_pretrained(MODEL_NAME, attention_type=\"original_full\").to(device)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(model.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK0wYT_MXjeI"
      },
      "source": [
        "Use preprocessed data and join them together for further works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1hfZLThQm5V"
      },
      "outputs": [],
      "source": [
        "transformer_documents = [' '.join(x) for x in all_tokens_nonstop_lemstem]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fzOcXkWX2Ho"
      },
      "source": [
        "Unfortunalely available models for transformers can not process documents with more than 512 characters. so we partition each document to sections with 512 or less characters. Afterwards, If our data length exceeds the specified limit, partition_doc function find their embeddings and calculate the average of the vectors as the final embeddeding for each document recursively. Otherwise we calculate the embedding simply by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDFLedX4M3lH"
      },
      "outputs": [],
      "source": [
        "def partition_doc(document):\n",
        "  x = []\n",
        "  def partition(document):\n",
        "    if len(document) == 0:\n",
        "      return x\n",
        "    t = tokenizer(document[:512], return_tensors='pt').to(device)\n",
        "    m = model(**t)[0].detach().squeeze()\n",
        "    x.append(torch.mean(m, dim=0))\n",
        "    return partition(document[512:])\n",
        "  z = partition(document)\n",
        "  return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV7oE3B9ghME"
      },
      "source": [
        "In this loop we tokenize the document and return it as PyTorch tensors and pass it onto the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H67w6hshoD0y",
        "outputId": "9df29149-54b0-42e6-c940-559b60285085"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n"
          ]
        }
      ],
      "source": [
        "averaged_vectors = []\n",
        "counter = 0 \n",
        "for document in transformer_documents:\n",
        "  try:\n",
        "    x = tokenizer(document, return_tensors='pt').to(device)\n",
        "    m = model(**x)[0].detach().squeeze()\n",
        "    averaged_vectors.append(torch.mean(m, dim=0))\n",
        "  except:\n",
        "    tensors = partition_doc(document)\n",
        "    result = torch.stack(tensors, dim=0).sum(dim=0)\n",
        "    averaged_vectors.append(torch.div(result, len(tensors)))\n",
        "  if counter % 100 == 0:\n",
        "    print(counter)\n",
        "  counter +=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZfrwyQejUDY",
        "outputId": "7dac5a49-1dd4-4a38-e0f7-fca6cd4fb8b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[torch.Size([768]),\n",
              " torch.Size([768]),\n",
              " torch.Size([768]),\n",
              " torch.Size([768]),\n",
              " torch.Size([768])]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[v.size() for v in averaged_vectors[:5]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbcGRMnTk7Ll",
        "outputId": "95c81ca1-6859-4968-d098-8559bdb1fd42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.7518, -0.2972,  0.5164,  ...,  0.9054, -1.0787, -0.7217],\n",
              "        [ 0.9634, -0.1464, -0.5599,  ...,  0.0650,  1.0207,  0.7631],\n",
              "        [-0.7309,  0.3546, -0.2604,  ...,  0.0146,  1.0128, -0.4903],\n",
              "        ...,\n",
              "        [ 0.4148, -0.3143, -0.1375,  ...,  0.5281,  0.2779, -1.3112],\n",
              "        [ 0.7779, -1.2898,  0.0992,  ...,  0.3979,  0.1694, -1.8513],\n",
              "        [ 1.7691, -0.2985,  0.4894,  ...,  0.8071,  0.4125,  0.0774]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = tokenizer(transformer_documents[1], return_tensors='pt').to(device)\n",
        "m = model(**x)[0].detach().squeeze()\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0c-kC4hg5NR"
      },
      "source": [
        "We install the requirements for using faiss and query and searching!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzgwIjpPcPzw",
        "outputId": "68eb3db4-89da-4bb4-d720-5f8c40b4e031"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libomp5\n",
            "Suggested packages:\n",
            "  libomp-doc\n",
            "The following NEW packages will be installed:\n",
            "  libomp-dev libomp5\n",
            "0 upgraded, 2 newly installed, 0 to remove and 62 not upgraded.\n",
            "Need to get 239 kB of archives.\n",
            "After this operation, 804 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp-dev amd64 5.0.1-1 [5,088 B]\n",
            "Fetched 239 kB in 1s (258 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 155639 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Selecting previously unselected package libomp-dev.\n",
            "Preparing to unpack .../libomp-dev_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp-dev (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp-dev (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install libomp-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n56GdjbmBudm"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW9d6xaWhft7"
      },
      "source": [
        "Index all the documents, we need them as numpy arrays first\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xDVCixNFuUm"
      },
      "outputs": [],
      "source": [
        "index = faiss.IndexIDMap(faiss.IndexFlatIP(768))\n",
        "index.add_with_ids(np.array([t.cpu().numpy() for t in averaged_vectors]), np.array(range(0, len(documents))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCL6x7WKh9TG"
      },
      "source": [
        "With *search* and *encoder* function we can encode queries into the vector space and find the most relevant documents to them with the help of faiss. Also we calculate the score of each document and return them with respect to ***k***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2QdDxKWlZpU"
      },
      "outputs": [],
      "source": [
        "def encoder(document: str) -> torch.Tensor:\n",
        "  tokens = tokenizer(document, return_tensors='pt').to(device)\n",
        "  vector = model(**tokens)[0].detach().squeeze()\n",
        "  return torch.mean(vector, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c12hHQ9OhrAb"
      },
      "outputs": [],
      "source": [
        "def search(query: str, k=12):\n",
        "  encoded_query = encoder(query).unsqueeze(dim=0).cpu().numpy()\n",
        "  top_k = index.search(encoded_query, k)\n",
        "  scores = top_k[0][0]\n",
        "  return top_k[1][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmnl8WEzkIGQ"
      },
      "source": [
        "In this section we can enter our input and after normalization and lemmatization of the query, we return top ***k*** relevant documents titles and links."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz3e6C12R--Q"
      },
      "outputs": [],
      "source": [
        "input = 'تپش قلب'\n",
        "splitted_input = input.split(\" \")\n",
        "nsi = []\n",
        "k = 12\n",
        "for x in splitted_input:\n",
        "  if x not in stopwords:\n",
        "    nsi.append(normalizer.normalize(lemmatizer.lemmatize(x))) \n",
        "\n",
        "input = \" \".join(nsi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk6Id7MsgSIU",
        "outputId": "291d15ec-50b3-4cfb-e43b-cb6cf0b4a5f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1206,  494,  460,  242,  211, 1885, 1575, 1966,  786,  255,  523,\n",
              "         19])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res = search(input, k)\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKK8f-nIdjaL",
        "outputId": "eed1567d-913e-4604-e3e3-d9e3bd4005b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "کودکی که سکته قلبی را از رو برد!\n",
            "https://www.hidoctor.ir/40534_%da%a9%d9%88%d8%af%da%a9%db%8c-%da%a9%d9%87-%d8%b3%da%a9%d8%aa%d9%87-%d9%82%d9%84%d8%a8%db%8c-%d8%b1%d8%a7-%d8%a7%d8%b2-%d8%b1%d9%88-%d8%a8%d8%b1%d8%af.html/\n",
            "\n",
            "علل و علائم و نشانه های ایست قلبی را باهم مرور میکنیم\n",
            "https://www.hidoctor.ir/10392_%d8%b9%d9%84%d9%84-%d9%88-%d8%b9%d9%84%d8%a7%d8%a6%d9%85-%d9%88-%d9%86%d8%b4%d8%a7%d9%86%d9%87-%d9%87%d8%a7%db%8c-%d8%a7%db%8c%d8%b3%d8%aa-%d9%82%d9%84%d8%a8%db%8c-%d8%b1%d8%a7-%d8%a8%d8%a7%d9%87.html/\n",
            "\n",
            "درباره گرفتگی عروق بیشتر بخوانید\n",
            "https://www.hidoctor.ir/212450_%d8%af%d8%b1%d8%a8%d8%a7%d8%b1%d9%87-%da%af%d8%b1%d9%81%d8%aa%da%af%db%8c-%d8%b9%d8%b1%d9%88%d9%82-%d8%a8%db%8c%d8%b4%d8%aa%d8%b1-%d8%a8%d8%ae%d9%88%d8%a7%d9%86%db%8c%d8%af.html/\n",
            "\n",
            "عوارض سندرم وحشتناک مغزی روی این دختر\n",
            "https://www.hidoctor.ir/40428_%d8%b9%d9%88%d8%a7%d8%b1%d8%b6-%d8%b3%d9%86%d8%af%d8%b1%d9%85-%d9%88%d8%ad%d8%b4%d8%aa%d9%86%d8%a7%da%a9-%d9%85%d8%ba%d8%b2%db%8c-%d8%b1%d9%88%db%8c-%d8%a7%db%8c%d9%86-%d8%af%d8%ae%d8%aa%d8%b1.html/\n",
            "\n",
            "کم خونی و علائم آن را بشناسیم\n",
            "https://www.hidoctor.ir/217980_%da%a9%d9%85-%d8%ae%d9%88%d9%86%db%8c-%d9%88-%d8%b9%d9%84%d8%a7%d8%a6%d9%85-%d8%a2%d9%86-%d8%b1%d8%a7-%d8%a8%d8%b4%d9%86%d8%a7%d8%b3%db%8c%d9%85.html/\n",
            "\n",
            "بیماری که زبان را دالبر می کند!\n",
            "https://namnak.com/بیماری-زبان.p47949\n",
            "\n",
            "رابطه کفش پاشنه بلند و سردرد خانم ها\n",
            "https://namnak.com/رابطه-کفش-پاشنه-بلند-سردرد-خانم-ها.p4284\n",
            "\n",
            "تیک های عصبی ناشی از کمبود چه موادی هستند\n",
            "https://namnak.com/علت-تیک-های-عصبی.p55960\n",
            "\n",
            "ب کمپلکسها باعث کاهش وزن مي شوند\n",
            "https://www.hidoctor.ir/355_%d8%a8-%da%a9%d9%85%d9%be%d9%84%da%a9%d8%b3%d9%87%d8%a7-%d8%a8%d8%a7%d8%b9%d8%ab-%da%a9%d8%a7%d9%87%d8%b4-%d9%88%d8%b2%d9%86-%d9%85%d9%8a-%d8%b4%d9%88%d9%86%d8%af.html/\n",
            "\n",
            "خطر داروهاي سرماخوردگي براي نوزادان\n",
            "https://www.hidoctor.ir/263_%d8%ae%d8%b7%d8%b1-%d8%af%d8%a7%d8%b1%d9%88%d9%87%d8%a7%d9%8a-%d8%b3%d8%b1%d9%85%d8%a7%d8%ae%d9%88%d8%b1%d8%af%da%af%d9%8a-%d8%a8%d8%b1%d8%a7%d9%8a-%d9%86%d9%88%d8%b2%d8%a7%d8%af%d8%a7%d9%86.html/\n",
            "\n",
            "آیا به قرص خواب نیاز دارید؟\n",
            "https://www.hidoctor.ir/433_%d8%a2%db%8c%d8%a7-%d8%a8%d9%87-%d9%82%d8%b1%d8%b5-%d8%ae%d9%88%d8%a7%d8%a8-%d9%86%db%8c%d8%a7%d8%b2-%d8%af%d8%a7%d8%b1%db%8c%d8%af%d8%9f.html/\n",
            "\n",
            "آیا داروهای فشار خون بر کارکرد قلب تاثیر دارند\n",
            "https://www.hidoctor.ir/72190_%d8%a2%db%8c%d8%a7-%d8%af%d8%a7%d8%b1%d9%88%d9%87%d8%a7%db%8c-%d9%81%d8%b4%d8%a7%d8%b1-%d8%ae%d9%88%d9%86-%d8%a8%d8%b1-%da%a9%d8%a7%d8%b1%da%a9%d8%b1%d8%af-%d9%82%d9%84%d8%a8-%d8%aa%d8%a7%d8%ab%db%8c.html/\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for x in res:\n",
        "  print(bioset[x]['title'])\n",
        "  print(bioset[x]['link'])\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMtSx2HVLG_K"
      },
      "source": [
        "# <strong><font color=#5F9EA0></span> fasttext </strong> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFJuRc0vTKUg"
      },
      "source": [
        "ّFirst we need to download, extract and load Fasttext word embedding model to use it in our code: \n",
        "\n",
        "*   Trained persian language dataset\n",
        "*   fasttext library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJAv-AXNLSqT",
        "outputId": "62dcc931-beda-4cae-a83e-5b16c6dc0a86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-07-10 13:42:43--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4502524724 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.fa.300.bin.gz.1’\n",
            "\n",
            "cc.fa.300.bin.gz.1  100%[===================>]   4.19G  49.3MB/s    in 96s     \n",
            "\n",
            "2022-07-10 13:44:20 (44.9 MB/s) - ‘cc.fa.300.bin.gz.1’ saved [4502524724/4502524724]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM2B_B3SUbDW",
        "outputId": "b88d6242-34de-409c-9719-65f5fddbc7f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 5.2 MB/s \n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/pybind11/\u001b[0m\n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.0-py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3164131 sha256=17d4eb757b6c12d1bd45db73c17ea5b3e1c7e0071cb5ad42b1efb440c40ff149\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.0\n"
          ]
        }
      ],
      "source": [
        "!gunzip ../content/cc.fa.300.bin.gz\n",
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0aQbFMofpg3"
      },
      "outputs": [],
      "source": [
        "import fasttext.util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfr8XptNV1GA"
      },
      "outputs": [],
      "source": [
        "fasttext.util.download_model('fa', if_exists='ignore')  # Persian\n",
        "ft = fasttext.load_model('cc.fa.300.bin')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig0L3B0gTkaT"
      },
      "source": [
        "We need numpy to calculate vector calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNPGjwDQd2JQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzrJG4AVjAKR"
      },
      "source": [
        "Now you need to Enter a Query that we with Fasttext method later find the best results. This query first will be pre-processed and then the vectors based on our trained dataset will be built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Aqb3kjIXCc-",
        "outputId": "d8130c58-1e82-4bd3-dcb1-1f3c4414e5a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "ft = fasttext.load_model('cc.fa.300.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfyCvaaoeqrq"
      },
      "outputs": [],
      "source": [
        "stopwords = [normalizer.normalize(x.strip()) for x in codecs.open('stopwords.txt','r','utf-8').readlines()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_Gr6IeWTk6o"
      },
      "outputs": [],
      "source": [
        "# print(\"Search: \", end='')\n",
        "# search = input()\n",
        "search = 'تپش قلب'\n",
        "search_words_splited = search.split(\" \")\n",
        "vectors = []\n",
        "\n",
        "search_words = []\n",
        "for x in search_words_splited:\n",
        "  if x not in stopwords:\n",
        "    search_words.append([normalizer.normalize(lemmatizer.lemmatize(x))])\n",
        "\n",
        "for word in search_words:\n",
        "  vectors.append(np.array(ft.get_word_vector(word[0])))\n",
        "\n",
        "vectors = np.array(vectors)\n",
        "search_vector = np.mean(vectors, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quBf3Fjvi2kC"
      },
      "source": [
        "To compare the query with our documents we will also need these docs' vectors.<br>\n",
        "So in this section all of the doc's vectors will be build."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdGRWVlzf66E"
      },
      "outputs": [],
      "source": [
        "doc_vectors = []\n",
        "for i in range(len(all_tokens_nonstop)):\n",
        "  vectors = []\n",
        "  for word in all_tokens_nonstop[i]:\n",
        "    vectors.append(np.array(ft.get_word_vector(word)))\n",
        "  vectors = np.array(vectors)\n",
        "  doc_vectors.append(np.mean(vectors, axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "602n9IVBjIWi"
      },
      "source": [
        "To see how close one doc is to the query we Find the cosine distant between Query-vector and each Doc-vectors.\n",
        "<br>\n",
        "<strong><font color=#cf6679>Also</span> \"k\" is set to be 10 you can change it yourself</strong> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NCE4DPukhTQ"
      },
      "outputs": [],
      "source": [
        "k = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skq8ueOYjJZN"
      },
      "outputs": [],
      "source": [
        "distance_list = []\n",
        "\n",
        "for doc_vector in doc_vectors:\n",
        "  distance_list.append(np.dot(search_vector, doc_vector)/(np.linalg.norm(search_vector)*np.linalg.norm(doc_vector)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T2q-3oklmf8"
      },
      "source": [
        "Now we sort the distant list and find the best matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbwEjYvHlmyL"
      },
      "outputs": [],
      "source": [
        "sorted_dist_list = []\n",
        "for doc_vec in distance_list:\n",
        "  sorted_dist_list.append(doc_vec)\n",
        "sorted_dist_list.sort(reverse=True)\n",
        "doc_indices = []\n",
        "for i in range(k):\n",
        "  doc_indices.append(distance_list.index(sorted_dist_list[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYfdEcCJf6oJ"
      },
      "source": [
        "Printing the information of best matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQSyV_8jndqn",
        "outputId": "c1507c2d-d37a-446d-f463-d6ade4dee2b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1769, 460, 272, 974, 1305, 26, 494, 1063, 2084, 1827]\n",
            "روش اندازه گیری نبض و عدد نرمال آن برای هر سن\n",
            "https://namnak.com/اندازه-گیری-نبض.p8605\n",
            "\n",
            "درباره گرفتگی عروق بیشتر بخوانید\n",
            "https://www.hidoctor.ir/212450_%d8%af%d8%b1%d8%a8%d8%a7%d8%b1%d9%87-%da%af%d8%b1%d9%81%d8%aa%da%af%db%8c-%d8%b9%d8%b1%d9%88%d9%82-%d8%a8%db%8c%d8%b4%d8%aa%d8%b1-%d8%a8%d8%ae%d9%88%d8%a7%d9%86%db%8c%d8%af.html/\n",
            "\n",
            "آشنایی بیشتر با داروی نیتروگلیسیرین\n",
            "https://www.hidoctor.ir/18046_%d8%a2%d8%b4%d9%86%d8%a7%db%8c%db%8c-%d8%a8%db%8c%d8%b4%d8%aa%d8%b1-%d8%a8%d8%a7-%d8%af%d8%a7%d8%b1%d9%88%db%8c-%d9%86%db%8c%d8%aa%d8%b1%d9%88%da%af%d9%84%db%8c%d8%b3%db%8c%d8%b1%db%8c%d9%86.html/\n",
            "\n",
            "سندروم مرگبار حساسیت به هیجان و شادی در این کودک\n",
            "https://www.hidoctor.ir/35818_%d8%b3%d9%86%d8%af%d8%b1%d9%88%d9%85-%d9%85%d8%b1%da%af%d8%a8%d8%a7%d8%b1-%d8%ad%d8%b3%d8%a7%d8%b3%db%8c%d8%aa-%d8%a8%d9%87-%d9%87%db%8c%d8%ac%d8%a7%d9%86-%d9%88-%d8%b4%d8%a7%d8%af%db%8c-%d8%af%d8%b1.html/\n",
            "\n",
            "تاثیر پیاده روی کردن در افزایش هوش\n",
            "https://namnak.com/افزایش-هوش.p52324\n",
            "\n",
            "ریتالین دشمن سلامتی قلب\n",
            "https://www.hidoctor.ir/142559_%d8%b1%db%8c%d8%aa%d8%a7%d9%84%db%8c%d9%86-%d8%af%d8%b4%d9%85%d9%86-%d8%b3%d9%84%d8%a7%d9%85%d8%aa%db%8c-%d9%82%d9%84%d8%a8.html/\n",
            "\n",
            "علل و علائم و نشانه های ایست قلبی را باهم مرور میکنیم\n",
            "https://www.hidoctor.ir/10392_%d8%b9%d9%84%d9%84-%d9%88-%d8%b9%d9%84%d8%a7%d8%a6%d9%85-%d9%88-%d9%86%d8%b4%d8%a7%d9%86%d9%87-%d9%87%d8%a7%db%8c-%d8%a7%db%8c%d8%b3%d8%aa-%d9%82%d9%84%d8%a8%db%8c-%d8%b1%d8%a7-%d8%a8%d8%a7%d9%87.html/\n",
            "\n",
            "بهبود تپش قلب با دمنوش بادرنجبویه\n",
            "https://www.hidoctor.ir/300409_%d8%a8%d9%87%d8%a8%d9%88%d8%af-%d8%aa%d9%be%d8%b4-%d9%82%d9%84%d8%a8-%d8%a8%d8%a7-%d8%af%d9%85%d9%86%d9%88%d8%b4-%d8%a8%d8%a7%d8%af%d8%b1%d9%86%d8%ac%d8%a8%d9%88%db%8c%d9%87.html/\n",
            "\n",
            "ورزش مفید برای فشار خون بالاها\n",
            "https://namnak.com/high-blood-pressure-exercises.p81809\n",
            "\n",
            "وقتی عاشق می شوید سر مغزتان چه می آید؟\n",
            "https://namnak.com/عشق-و-مغز.p58455\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(doc_indices)\n",
        "for i in range(k):\n",
        "  print(bioset[doc_indices[i]]['title'])\n",
        "  print(bioset[doc_indices[i]]['link'])\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2AdjFRM0Lzl"
      },
      "source": [
        "# <strong><font color=#5F9EA0></span> Comparison between methods </strong> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s77GcIax2GG_"
      },
      "source": [
        "**According to the MRR rank:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTHeI7ee2WNX"
      },
      "source": [
        "The best method is **tf-idf**. It has performed better than other methods in terms of both mean scores and opinions. The **transformer** method was expected to be better than the other methods, but due to the fact that the processed language is Persian, the existing models for Persian transformers perform much weaker than English, and there were limitations in using them for this volume. Also, in general, the **fasttext** method works better than the regular **boolean** method, but here, by modifying the **boolean** method in our code and paying attention to the number of words in addition to their presence or absence, we made these two methods work almost identically, which shows the effect of our work and the changes which we applied to the **boolean** method.\n",
        "\n",
        "So in conclusion in our project the following relationship is established in terms of the efficiency of the methods:\n",
        "\n",
        "**tf-idf** > **modified boolean** = **fasttext** > **boolean** > **transformer**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Dul1eni14UhG",
        "Q2AdjFRM0Lzl"
      ],
      "name": "MIR_HW3.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}